<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Shubham Dokania</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Principal Component Analysis">
    <meta name="author" content="Shubham Dokania">
    <meta name="keywords" content="blog, pca">
    <link rel="canonical" href="http://shubham1810.github.io/blog/pca/2018/06/28/pca/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Shubham Dokania" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201907230021" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Verifications -->
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Principal Component Analysis">
    <meta property="og:description" content="Personal website and blog">
    <meta property="og:url" content="http://shubham1810.github.io/blog/pca/2018/06/28/pca/">
    <meta property="og:site_name" content="Shubham Dokania">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
        <meta name="twitter:site" content="@shubhamdokania" />
    
    <meta name="twitter:title" content="Principal Component Analysis" />
    <meta name="twitter:description" content="Principal Component Analysis" />
    <meta name="twitter:url" content="http://shubham1810.github.io/blog/pca/2018/06/28/pca/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-62090117-1']);
      _gaq.push(['_trackPageview']);
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    
</head>

<body class="site animated fade-in-down">
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://shubham1810.github.io" class="site-title"></a>
      <nav class="site-nav">
        <a href="/">Home</a>
<a href="/blog/">Blog</a>
      </nav>
      <div class="clearfix" ></div>
      
        <div class="social-icons">
  <div class="left">
    
      <a class="fa fa-github" href="https://github.com/shubham1810"></a>
    
    
    
    <a class="fa fa-rss" href="/feed.xml"></a>
    
      <a class="fa fa-twitter" href="https://twitter.com/shubhamdokania"></a>
    
    
      <a class="fa fa-google-plus" href="https://plus.google.com/+shubamkumar/posts"></a>
    
    
      <a class="fa fa-linkedin" href="https://www.linkedin.com/in/shubhamdokania"></a>
    
    
    
  </div>
  <div class="right">

    
    
    
  </div>
</div>

<div class="clearfix">
    
      <center> <a class="fa fa-envelope" href="mailto:shubham.k.dokania@gmail.com"></a> shubham.k.dokania@gmail.com <center>
    
</div>

    

      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>Principal Component Analysis</h1>
  <span class="post-meta">Jun 28, 2018</span><br>
  
  <span class="post-meta small">
  
    5 minute read
  
  </span>
</div>

<article class="post-content">
  <p>As we work with real world data, we notice that the complexity increases; both in terms of dependency of variables on each other and dimensionality (number of variables) of the problem. Several techniques exist for analysis of such information and to make it easier to extract important properties for the purpose of better computation and visualization. One such method is the <strong>Principal Component Analysis (PCA)</strong>, which emphasises on the variance of the data to extract the directions which maximize the data variation.</p>

<p>One of the major applications of PCA is dimensionality reduction, which is attained by choosing the transformed variables (obtained from projection of original variables on the direction of maximum variances, or the <em>principal components</em>).</p>

<p>Few of the prerequisites for understanding PCA are: <a href="https://en.wikipedia.org/wiki/Covariance_matrix"><em>Covariance</em></a>, <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>Eigenvectors</em></a>, and <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition"><em>Singular Value Decomposition</em></a>.</p>

<blockquote>
  <p>Note: Some resources to read about the aforementioned topics:</p>

  <ol>
    <li>Eigenvalues &amp; Eigenvectors: <a href="http://setosa.io/ev/eigenvectors-and-eigenvalues/">Setosa visualization</a>, <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g&amp;t=0s&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;index=15">3Blue1Brown</a></li>
    <li>SVD: <a href="https://medium.com/the-andela-way/foundations-of-machine-learning-singular-value-decomposition-svd-162ac796c27d">This nice Medium blogpost</a></li>
  </ol>
</blockquote>

<p>For example, take some data (Say, <script type="math/tex">X</script>) with zero mean (if mean is not zero then subtract all values <script type="math/tex">x_i</script> with the mean, <script type="math/tex">\mu</script>). The covariance of this data (Say <script type="math/tex">C_X</script>) is given by:</p>

<script type="math/tex; mode=display">C_X = \frac{1}{n}\cdot X\cdot X^T</script>

<p><img src="/images/PCA/data.png" alt="data" /></p>

<p>We want to figure out a transformation function <script type="math/tex">W</script> and apply on the data <script type="math/tex">X</script> so that in the resulting data <script type="math/tex">Y</script>, the variables will be independent of each other. In simple terms, the covariance between any two distinct columns of <script type="math/tex">Y</script> will be zero, i.e. the non-diagonal elements of the covariance matrix <script type="math/tex">C_Y</script> of <script type="math/tex">Y</script> will be zero.
This implies that <script type="math/tex">C_Y</script> will be a diagonal matrix.</p>

<p>Writing the transformation from <script type="math/tex">X</script> to <script type="math/tex">Y</script>, we have:</p>

<script type="math/tex; mode=display">Y = X\cdot W</script>

<p>To solve for the covariance matrix of Y, we can write</p>

<script type="math/tex; mode=display">C_Y = \frac{1}{n}\cdot Y\cdot Y^T</script>

<p>and since, <script type="math/tex">Y = W\cdot X</script>, we have,</p>

<script type="math/tex; mode=display">C_Y = \frac{1}{n}\cdot W\cdot X\cdot (W\cdot X)^T\\
C_Y = \frac{1}{n}\cdot W\cdot X\cdot X^T\cdot W^T\\
C_Y = W\cdot (\frac{1}{n}\cdot X\cdot X^T)\cdot W^T\\
C_Y = W\cdot C_X\cdot W^T</script>

<p>or,</p>

<script type="math/tex; mode=display">C_X = W^T\cdot C_Y\cdot W</script>

<p>We know that, <script type="math/tex">C_Y</script> is supposed to be a diagonal matrix. What does this equation remind us of? <em>but of course</em>, the Singular Value Decomposition (SVD).
Thus, If we take <script type="math/tex">W</script> as the matrix of the eigenvectors and <script type="math/tex">C_Y</script> as the diagonal matrix of the eigenvalues, the above equation will hold true, making the matrix <script type="math/tex">W</script>, of eigenvectors of covariance of <script type="math/tex">X</script>, our transformation matrix.</p>

<p>Computing the above values for our data, and plotting the directions of the obtained eigenvalues, we get the following:</p>

<p><img src="/images/PCA/eig.png" alt="eig" /></p>

<p>As can be seen clearly, one of the eigenvectors falls along the direction of maximum variance of the data. On transforming the data <script type="math/tex">X</script> into <script type="math/tex">Y</script>, and plotting again, we get:</p>

<p><img src="/images/PCA/trans.png" alt="trans" /></p>

<p>Printing the covariance of the new data <script type="math/tex">Y</script>, we can see itâ€™s a diagonal matrix. Also, the equation <script type="math/tex">W\cdot C_Y\cdot W^T</script> returns the original covariance matrix <script type="math/tex">C_X</script>.</p>

<p><img src="/images/PCA/sig.png" alt="sig" /></p>

<h3 id="dimension-reduction">Dimension Reduction</h3>

<p>One of the major applications of PCA is itâ€™s ability to choose the dimensions of maximum variation, i.e. taking the projection of the data along those components only will not affect the complexity of the data by a significant amount and data can be reconstructed back to an approximation of itâ€™s original form with the lower dimensional data as well.</p>

<p>On paying more attention to the covariance matrix <script type="math/tex">C_Y</script>, we see that the magnitude of the eigenvalues along the diagonal of the matrix is related to the amount of variances explained by the said eigenvector direction.</p>

<p>So, sorting the eigenvalues and corresponding eigenvector pairs in decreasing order and taking only the top values becomes the ideal way of choosing the eigenvectors for obtaining maximum explained variances.</p>

<p>For further demonstration, letâ€™s use another dataset (<a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>) for PCA.</p>

<p><img src="/images/PCA/mnist.png" alt="mnist" /></p>

<p>Computing the eigenvectors and eigenvalues for the above dataset and sorting them on the basis of eigenvalues (descending order), we can store them back in numpy arrays.</p>

<p><img src="/images/PCA/mnist_eig.png" alt="mnist_eig" /></p>

<p>And plot the eigenvalues, and the cumulative sum of the eigenvalues (<strong>Explained Variances</strong>).</p>

<p><img src="/images/PCA/exp_var.png" alt="exp_var" /></p>

<p>From the above curve for the cumulative sum, denoting the explained variances of the original data, we can conclude that approximate 150 dimensions shall be enough to get ~95% of the variances of the original dataset, and about 326 dimensions out of 784 for ~99%.</p>

<p>To reduce the number of dimensions, we have to select the number of dimensions we want <script type="math/tex">k</script> and use only those <script type="math/tex">k</script> columns from <script type="math/tex">W</script> to form the transformation matrix (Say <script type="math/tex">W'</script>). Thus the transformation and reconstruction operation become:</p>

<script type="math/tex; mode=display">Y_{m \times k} = X_{m \times n} \cdot W'_{n \times k}\\
\\
X'_{m \times n} = Y_{m \times k} \cdot W'^T_{k \times n}</script>

<p>Letâ€™s now pick only 2 dimensions (~23% explained variance), and plot the points as a scatter plot, and color based on the class label from the training set. Letâ€™s use scikit-learn package for this last operation:</p>

<p><img src="/images/PCA/pca_data.png" alt="pca_data" /></p>

<p>From the scatter plot, we can do some simple analysis and see some relationship between the color of points (labels) and their location on the plot. For instance, the green cluster (representing the label <strong>1</strong>) is formed clearly distinct from others, while the clusters for colors brown and pink (for digits <strong>4</strong> and <strong>9</strong>) are somewhat in the same region, etc.</p>

<p>Although the explained variance with 2 dimensions was roughly 23%, we still can derive some meaningful information about the data. Having more number of dimensions will make it easier to process and analyse the data as compared to the original data distribution.</p>

<p>Also, applying PCA would make it easier to use the data in models such as the <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>, where the core assumption is that the columns are independent of each other.</p>

<blockquote>
  <p>Note: If we want to keep the physical meaning of the columns in the dataset intact, using PCA would be a bad idea since the transformed columns are linear combinations of the original columns. Hence, the new columns would lose their original meaning.</p>

  <p>Also, dimension reduction is useful only if the eigenvalues vary significantly for any data distribution. For eigenvalues in similar ranges, each column will have similar contribution towards the variation in data, hence removing them would cause greater loss.</p>
</blockquote>

<p>â€“</p>


</article>


  





  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'shubhamdokania';
    var disqus_identifier = '/blog/pca/2018/06/28/pca';
    var disqus_title      = '';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Powered by <a href="https://jekyllrb.com/">Jekyll</a> using <a href="https://pixyll.com/">Pixyll</a> theme. Hosted on <a href="https://pages.github.com/">Github Pages</a>. <br>
      Â© Shubham Dokania
    </small>
  </div>
</footer>

</body>
</html>
